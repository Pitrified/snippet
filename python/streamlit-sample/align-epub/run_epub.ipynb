{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and use the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    util,\n",
    ")\n",
    "import spacy\n",
    "\n",
    "from cached_pipe import PipelineCache\n",
    "from epub import (\n",
    "    EPub,\n",
    "    Chapter,\n",
    ")\n",
    "from utils import (\n",
    "    get_ebook_folder,\n",
    "    enumerate_sent,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NLP objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lts = [\"en\", \"fr\"]\n",
    "lts_pair = list(zip(lts, lts[::-1]))\n",
    "lts, lts_pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = {\n",
    "    \"en\": spacy.load(\"en_core_web_md\"),\n",
    "    \"fr\": spacy.load(\"fr_core_news_md\"),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Should export env variables to avoid needing an internet connection.\n",
    "# https://huggingface.co/docs/transformers/installation#offline-mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = {\n",
    "    f\"{lt}_{lt_other}\": pipeline(\n",
    "        \"translation\", model=f\"Helsinki-NLP/opus-mt-{lt}-{lt_other}\"\n",
    "    )\n",
    "    for lt, lt_other in lts_pair\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cached translator pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_file_path = {\n",
    "    f\"{lt}_{lt_other}\": Path(f\"translated_{lt}_{lt_other}.json\")\n",
    "    for lt, lt_other in lts_pair\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_cache = {\n",
    "    (lt_pair := f\"{lt}_{lt_other}\"): PipelineCache(\n",
    "        pipe[lt_pair], cache_file_path[lt_pair], lt, lt_other\n",
    "    )\n",
    "    for lt, lt_other in lts_pair\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_cache[\"en_fr\"](\"Let's try this cool way to create a callable class.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load epubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebook_folder = get_ebook_folder()\n",
    "epub_path = {\n",
    "    \"fr\": ebook_folder / \"Gaston_Leroux_-_Le_Mystere_de_la_chambre_jaune.epub\",\n",
    "    \"en\": ebook_folder / \"mystery_yellow_room.epub\",\n",
    "}\n",
    "print(epub_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epub = {\n",
    "    lt: EPub(epub_path[lt], nlp, pipe_cache, lt, lt_other)\n",
    "    for lt, lt_other in zip(lts, lts[::-1])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate a manazza and check for similarity with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_fr = epub[\"fr\"].chapters[0].paragraphs[0].sents_orig[0]\n",
    "sent_fr.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_fr_to_en = pipe[\"fr_en\"](sent_fr.text)\n",
    "sent_fr_to_en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_fr_to_en = nlp[\"en\"](sent_fr_to_en[0][\"translation_text\"])\n",
    "print(type(doc_fr_to_en))\n",
    "doc_fr_to_en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_en = epub[\"en\"].chapters[0].paragraphs[2].sents_orig[0]\n",
    "print(type(sent_en))\n",
    "sent_en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_fr_to_en.similarity(sent_en)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_en2 = epub[\"en\"].chapters[0].paragraphs[2].sents_orig[2]\n",
    "print(sent_en2)\n",
    "\n",
    "doc_fr_to_en.similarity(sent_en2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate over sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def enumerate_sent(\n",
    "#     chap: Chapter,\n",
    "#     start_par: int = 0,\n",
    "#     end_par: int = 0,\n",
    "#     which_sent=\"orig\",\n",
    "# ):\n",
    "#     \"\"\"\"\"\"\n",
    "#     if end_par == 0:\n",
    "#         end_par = len(chap.paragraphs) + 1\n",
    "#     for i_p, par in enumerate(chap.paragraphs[start_par:end_par]):\n",
    "#         for i_s, sent in enumerate(par.sents_orig):\n",
    "#             if which_sent == \"orig\":\n",
    "#                 yield (i_p + start_par, i_s), sent\n",
    "#             elif which_sent == \"tran\":\n",
    "#                 yield (i_p + start_par, i_s), par.sents_tran[i_s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_delta = 2\n",
    "\n",
    "sent_text_en = []\n",
    "for k, sent in enumerate_sent(epub[\"en\"].chapters[0+ch_delta], which_sent=\"orig\"):\n",
    "    text_en = sent.text\n",
    "    # print(k, text_en)\n",
    "    sent_text_en.append(text_en)\n",
    "\n",
    "sent_text_fr_tran = []\n",
    "for k, sent in enumerate_sent(epub[\"fr\"].chapters[0+ch_delta], which_sent=\"tran\"):\n",
    "    text_fr_tran = sent.text\n",
    "    sent_text_fr_tran.append(text_fr_tran)\n",
    "\n",
    "print(sent_text_en[4])\n",
    "print(sent_text_fr_tran[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence encoder used for similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_transformer = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "sentence_transformer = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Our sentences we like to encode\n",
    "sentences = [\n",
    "    \"This framework generates embeddings for each input sentence\",\n",
    "    \"Sentences are passed as a list of string.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "]\n",
    "\n",
    "# Sentences are encoded by calling sentence_transformer.encode()\n",
    "embeddings = sentence_transformer.encode(sentences)\n",
    "\n",
    "# # Print the embeddings\n",
    "# for sentence, embedding in zip(sentences, embeddings):\n",
    "#     print(\"Sentence:\", sentence)\n",
    "#     print(\"Embedding:\", embedding)\n",
    "#     print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"I'm happy\", \"I'm full of happiness\"]\n",
    "\n",
    "\n",
    "# Compute embedding for both lists\n",
    "embedding_1 = sentence_transformer.encode(sentences[0], convert_to_tensor=True)\n",
    "embedding_2 = sentence_transformer.encode(sentences[1], convert_to_tensor=True)\n",
    "\n",
    "util.pytorch_cos_sim(embedding_1, embedding_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_1 = sentence_transformer.encode(sent_en.text, convert_to_tensor=True)\n",
    "embedding_2 = sentence_transformer.encode(sent_en2.text, convert_to_tensor=True)\n",
    "embedding_fr = sentence_transformer.encode(doc_fr_to_en.text, convert_to_tensor=True)\n",
    "fr1 = util.pytorch_cos_sim(embedding_1, embedding_fr)\n",
    "fr2 = util.pytorch_cos_sim(embedding_2, embedding_fr)\n",
    "fr1, fr2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert sents to embedding and compute the distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_num_en = len(sent_text_en)\n",
    "enc_en = sentence_transformer.encode(sent_text_en, convert_to_tensor=True)\n",
    "print(\"en\", enc_en.shape, sent_num_en, enc_en[0].shape)\n",
    "\n",
    "sent_num_fr = len(sent_text_fr_tran)\n",
    "enc_fr_tran = sentence_transformer.encode(sent_text_fr_tran, convert_to_tensor=True)\n",
    "print(\"fr\", enc_fr_tran.shape, sent_num_fr, enc_fr_tran[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim = np.zeros((sent_num_en, sent_num_fr))\n",
    "# for i in range(sent_num_en):\n",
    "#     for ii in range(sent_num_fr):\n",
    "#         sim[i][ii] = util.pytorch_cos_sim(enc_en[i], enc_fr_tran[ii])\n",
    "#         # sim[i][ii] = util.pytorch_cos_sim(enc_en[i], enc_en[ii])\n",
    "# plt.imshow(sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_torch = util.pytorch_cos_sim(enc_en, enc_fr_tran)\n",
    "sim = sim_torch.detach().cpu().numpy()\n",
    "plt.imshow(sim)\n",
    "plt.title(f\"Similarity *en* vs *fr_translated*\")\n",
    "plt.ylabel(\"en\")\n",
    "plt.xlabel(\"fr_tran\")\n",
    "plt.savefig(f\"Similarity_en_vs_fr_translated_{ch_delta}.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_len = 20\n",
    "ratio = sent_num_en / sent_num_fr\n",
    "print(f\"{ratio=}\")\n",
    "sim_center = np.zeros((sent_num_en, win_len * 2 + 1))\n",
    "for i in range(sent_num_en):\n",
    "    # the similarity of this english sent to all the translated ones\n",
    "    this_sent_sim = sim[i]\n",
    "    # find the center rescaled because there are different number of sent in the two chapters\n",
    "    ii = int(i / ratio)\n",
    "    if ii < win_len:\n",
    "        ii = win_len\n",
    "    if ii > sent_num_fr - (win_len + 1):\n",
    "        ii = sent_num_fr - (win_len + 1)\n",
    "    # the chopped similarity array\n",
    "    some_sent_sim = this_sent_sim[ii - win_len : ii + win_len + 1]\n",
    "    sim_center[i] = some_sent_sim\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(sim_center, aspect=\"auto\")\n",
    "ax.set_title(f\"Similarity en vs fr_tran, shifted\")\n",
    "ax.set_ylabel(\"en\")\n",
    "ax.set_xlabel(\"fr_tran shifted\")\n",
    "\n",
    "# I hate matplotlib\n",
    "\n",
    "# overlap\n",
    "# xticks_lab = list(range(-win_len, win_len + 1))\n",
    "# xticks_pos = list(range(win_len * 2))\n",
    "\n",
    "# works but I hate it\n",
    "\n",
    "# xticks_lab = list(range(-win_len, win_len + 1, 3))\n",
    "# xticks_pos = list(range(0, win_len * 2 + 1, 3))\n",
    "\n",
    "step = 3\n",
    "xticks_lab = list(range(-step, -win_len, -step))[::-1] + list(range(0, win_len, step))\n",
    "min_lab = xticks_lab[0]\n",
    "min_shift = win_len + min_lab\n",
    "xticks_pos = list(range(min_shift, win_len * 2 + 1, step))\n",
    "\n",
    "ax.set_xticks(xticks_pos)\n",
    "ax.set_xticklabels(xticks_lab)\n",
    "\n",
    "print(ax.xaxis.get_ticklabels())\n",
    "print(ax.get_xticks())\n",
    "\n",
    "# ax.xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "\n",
    "plt.savefig(f\"Similarity_en_vs_fr_tran_shifted_{ch_delta}.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(range(-win_len, 0, 3))\n",
    "step = 3\n",
    "lab = list(range(-step, -win_len, -step))[::-1] + list(range(0, win_len, step))\n",
    "print(lab)\n",
    "min_lab = lab[0]\n",
    "print(min_lab)\n",
    "min_shift = win_len + min_lab\n",
    "print(min_shift)\n",
    "pos = list(range(min_shift, win_len * 2 + 1, step))\n",
    "print(pos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use similarity to pair up sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim[0].shape, sim[:, 0].shape, sim.shape\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "44d69056cbf37a415d1103e29d5d1df97d82e2a69c9f4e12d2363bb333fa02eb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('hug')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
