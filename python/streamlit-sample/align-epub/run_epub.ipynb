{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and use the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import cast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "from scipy.signal.windows import triang\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers.pipelines import pipeline\n",
    "from transformers.pipelines.text2text_generation import TranslationPipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from cached_pipe import TranslationPipelineCache\n",
    "from epub import Chapter, EPub\n",
    "from utils import enumerate_sent, get_ebook_folder, spacy_load_cached, sentence_encode_np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NLP objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lts = [\"en\", \"fr\"]\n",
    "lts_pair = list(zip(lts, lts[::-1]))\n",
    "lts_pair_tags = [f\"{lt}_{lt_other}\" for lt, lt_other in lts_pair]\n",
    "lts, lts_pair, lts_pair_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = Path(\"~/.cache/spacy_my_models\").expanduser()\n",
    "\n",
    "nlp = {\n",
    "    # \"en\": spacy.load(\"en_core_web_md\"),\n",
    "    \"en\": spacy_load_cached(\"en_core_web_md\", cache_dir),\n",
    "    # \"fr\": spacy.load(\"fr_core_news_md\"),\n",
    "    \"fr\": spacy_load_cached(\"fr_core_news_md\", cache_dir),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Should export env variables to avoid needing an internet connection.\n",
    "# https://huggingface.co/docs/transformers/installation#offline-mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pipeline only if needed, both models take 3.4Gb of GPU memory :(\n",
    "load_pipeline = {\n",
    "    \"en\": False,\n",
    "    \"fr\": False,\n",
    "}\n",
    "\n",
    "pipe = {\n",
    "    f\"{lt}_{lt_other}\": cast(\n",
    "        TranslationPipeline,\n",
    "        pipeline(\"translation\", model=f\"Helsinki-NLP/opus-mt-{lt}-{lt_other}\"),\n",
    "    )\n",
    "    if load_pipeline[lt]\n",
    "    else None\n",
    "    for lt, lt_other in lts_pair\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_transformer = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "sentence_transformer = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cached translator pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_file_path = {\n",
    "    f\"{lt}_{lt_other}\": Path(f\"translated_{lt}_{lt_other}.json\")\n",
    "    for lt, lt_other in lts_pair\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_cache = {\n",
    "    (lt_pair := f\"{lt}_{lt_other}\"): TranslationPipelineCache(\n",
    "        pipe[lt_pair], cache_file_path[lt_pair], lt, lt_other\n",
    "    )\n",
    "    for lt, lt_other in lts_pair\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_cache[\"en_fr\"](\"Let's try this cool way to create a callable class.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load epubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebook_folder = get_ebook_folder()\n",
    "epub_path = {\n",
    "    \"fr\": ebook_folder / \"Gaston_Leroux_-_Le_Mystere_de_la_chambre_jaune.epub\",\n",
    "    \"en\": ebook_folder / \"mystery_yellow_room.epub\",\n",
    "}\n",
    "print(epub_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epub = {\n",
    "    lt: EPub(epub_path[lt], nlp, pipe_cache, lt, lt_other)\n",
    "    for lt, lt_other in zip(lts, lts[::-1])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Translate a manazza and check for similarity with spacy\n",
    "\n",
    "# sent_fr = epub[\"fr\"].chapters[0].paragraphs[0].sents_orig[0]\n",
    "# sent_fr.text\n",
    "\n",
    "# sent_fr_to_en = pipe[\"fr_en\"](sent_fr.text)\n",
    "# sent_fr_to_en\n",
    "\n",
    "# doc_fr_to_en = nlp[\"en\"](sent_fr_to_en[0][\"translation_text\"])\n",
    "# print(type(doc_fr_to_en))\n",
    "# doc_fr_to_en\n",
    "\n",
    "# sent_en = epub[\"en\"].chapters[0].paragraphs[2].sents_orig[0]\n",
    "# print(type(sent_en))\n",
    "# sent_en\n",
    "\n",
    "# doc_fr_to_en.similarity(sent_en)\n",
    "\n",
    "# sent_en2 = epub[\"en\"].chapters[0].paragraphs[2].sents_orig[2]\n",
    "# print(sent_en2)\n",
    "\n",
    "# doc_fr_to_en.similarity(sent_en2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate over sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_id = 0\n",
    "ch_delta = 0\n",
    "\n",
    "ch_en = epub[\"en\"].chapters[ch_id + ch_delta]\n",
    "ch_fr = epub[\"fr\"].chapters[ch_id]\n",
    "\n",
    "# sent_text_en = []\n",
    "# sent_doc_en = []\n",
    "# for k, sent in enumerate_sent(ch_en, which_sent=\"orig\"):\n",
    "#     text_en = sent.text\n",
    "#     # print(k, text_en)\n",
    "#     sent_text_en.append(text_en)\n",
    "#     sent_doc_en.append(sent)\n",
    "\n",
    "# sent_text_fr_tran = []\n",
    "# sent_doc_fr_tran = []\n",
    "# for k, sent in enumerate_sent(ch_fr, which_sent=\"tran\"):\n",
    "#     text_fr_tran = sent.text\n",
    "#     sent_text_fr_tran.append(text_fr_tran)\n",
    "#     sent_doc_fr_tran.append(sent)\n",
    "\n",
    "sent_text_en = ch_en.sents_text_orig\n",
    "sent_text_fr_tran = ch_fr.sents_text_tran\n",
    "\n",
    "print(sent_text_en[4])\n",
    "print(ch_en.sents_text_orig[4])\n",
    "print(sent_text_fr_tran[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence encoder used for similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_transformer = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "# sentence_transformer = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Our sentences we like to encode\n",
    "sentences = [\n",
    "    \"This framework generates embeddings for each input sentence\",\n",
    "    \"Sentences are passed as a list of string.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "]\n",
    "\n",
    "# Sentences are encoded by calling sentence_transformer.encode()\n",
    "embeddings = sentence_transformer.encode(sentences)\n",
    "\n",
    "# # Print the embeddings\n",
    "# for sentence, embedding in zip(sentences, embeddings):\n",
    "#     print(\"Sentence:\", sentence)\n",
    "#     print(\"Embedding:\", embedding)\n",
    "#     print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert sents to embedding and compute the distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_num_en = len(sent_text_en)\n",
    "# enc_en = cast(\n",
    "#     torch.Tensor,\n",
    "#     sentence_transformer.encode(sent_text_en, convert_to_tensor=True),\n",
    "# )\n",
    "# print(\"en\", enc_en.shape, sent_num_en, enc_en[0].shape)\n",
    "\n",
    "# sent_num_fr = len(sent_text_fr_tran)\n",
    "# enc_fr_tran = cast(\n",
    "#     torch.Tensor,\n",
    "#     sentence_transformer.encode(sent_text_fr_tran, convert_to_tensor=True),\n",
    "# )\n",
    "# print(\"fr\", enc_fr_tran.shape, sent_num_fr, enc_fr_tran[0].shape)\n",
    "\n",
    "# sent_num_en, ch_en.sents_num\n",
    "# # chop off the horrible chapter 0 beginning\n",
    "# sent_text_en_cheat = sent_text_en[3:]\n",
    "# sent_num_en_cheat = len(sent_text_en_cheat)\n",
    "# enc_en_cheat = cast(\n",
    "#     torch.Tensor,\n",
    "# sentence_transformer.encode(sent_text_en_cheat, convert_to_tensor=True),\n",
    "# )\n",
    "# # sim = np.zeros((sent_num_en, sent_num_fr))\n",
    "# # for i in range(sent_num_en):\n",
    "# #     for ii in range(sent_num_fr):\n",
    "# #         sim[i][ii] = util.pytorch_cos_sim(enc_en[i], enc_fr_tran[ii])\n",
    "# #         # sim[i][ii] = util.pytorch_cos_sim(enc_en[i], enc_en[ii])\n",
    "# # plt.imshow(sim)\n",
    "\n",
    "# sim_torch = util.pytorch_cos_sim(enc_en, enc_fr_tran)\n",
    "# sim = sim_torch.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "# enc_en_np = enc_en.detach().cpu().numpy()\n",
    "# enc_fr_tran_np = enc_fr_tran.detach().cpu().numpy()\n",
    "# sim_np = cosine_similarity(enc_en_np, enc_fr_tran_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do everything directly in numpy\n",
    "enc_en_np_direct = sentence_encode_np(sentence_transformer, sent_text_en)\n",
    "enc_fr_np_direct = sentence_encode_np(sentence_transformer, sent_text_fr_tran)\n",
    "sim = cosine_similarity(enc_en_np_direct, enc_fr_np_direct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sim)\n",
    "plt.title(f\"Similarity *en* vs *fr_translated*\")\n",
    "plt.ylabel(\"en\")\n",
    "plt.xlabel(\"fr_tran\")\n",
    "plt.savefig(f\"Similarity_en_vs_fr_translated_{ch_delta}.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_num_en = ch_en.sents_num\n",
    "sent_num_fr = ch_fr.sents_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_len = 20\n",
    "ratio = sent_num_en / sent_num_fr\n",
    "print(f\"{ratio=}\")\n",
    "sim_center = np.zeros((sent_num_en, win_len * 2 + 1))\n",
    "for i in range(sent_num_en):\n",
    "    # the similarity of this english sent to all the translated ones\n",
    "    this_sent_sim = sim[i]\n",
    "    # find the center rescaled because there are different number of sent in the two chapters\n",
    "    ii = int(i / ratio)\n",
    "    if ii < win_len:\n",
    "        ii = win_len\n",
    "    if ii > sent_num_fr - (win_len + 1):\n",
    "        ii = sent_num_fr - (win_len + 1)\n",
    "    # the chopped similarity array\n",
    "    some_sent_sim = this_sent_sim[ii - win_len : ii + win_len + 1]\n",
    "    sim_center[i] = some_sent_sim\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(sim_center, aspect=\"auto\")\n",
    "ax.set_title(f\"Similarity en vs fr_tran, shifted\")\n",
    "ax.set_ylabel(\"en\")\n",
    "ax.set_xlabel(\"fr_tran shifted\")\n",
    "\n",
    "# I hate matplotlib\n",
    "\n",
    "# overlap\n",
    "# xticks_lab = list(range(-win_len, win_len + 1))\n",
    "# xticks_pos = list(range(win_len * 2))\n",
    "\n",
    "# works but I hate it\n",
    "\n",
    "# xticks_lab = list(range(-win_len, win_len + 1, 3))\n",
    "# xticks_pos = list(range(0, win_len * 2 + 1, 3))\n",
    "\n",
    "step = 3\n",
    "xticks_lab = list(range(-step, -win_len, -step))[::-1] + list(range(0, win_len, step))\n",
    "min_lab = xticks_lab[0]\n",
    "min_shift = win_len + min_lab\n",
    "xticks_pos = list(range(min_shift, win_len * 2 + 1, step))\n",
    "\n",
    "ax.set_xticks(xticks_pos)\n",
    "ax.set_xticklabels(xticks_lab)\n",
    "\n",
    "print(ax.xaxis.get_ticklabels())\n",
    "print(ax.get_xticks())\n",
    "\n",
    "# ax.xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "\n",
    "plt.savefig(f\"Similarity_en_vs_fr_tran_shifted_{ch_delta}.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_max = []\n",
    "\n",
    "all_good_max = []\n",
    "all_good_i = []\n",
    "\n",
    "ratio = sent_num_en / sent_num_fr\n",
    "# ratio = sent_num_en_cheat / sent_num_fr\n",
    "\n",
    "for i in range(sent_num_en):\n",
    "    # for i in range(sent_num_en_cheat):\n",
    "    # for i in range(40):\n",
    "\n",
    "    # the similarity of this english sent to all the translated ones\n",
    "    this_sent_sim = sim[i]\n",
    "    # this_sent_sim = sim_cheat[i]\n",
    "\n",
    "    # find the center rescaled because there are different number of sent in the two chapters\n",
    "    ii = int(i / ratio)\n",
    "    # the chopped similarity array\n",
    "    win_left = max(0, ii - win_len)\n",
    "    win_right = min(sent_num_fr, ii + win_len + 1)\n",
    "    some_sent_sim = this_sent_sim[win_left:win_right]\n",
    "    # print(f\"{i} {ii} {ii-win_len} {ii+win_len+1} {some_sent_sim}\")\n",
    "\n",
    "    max_id = some_sent_sim.argmax() + win_left\n",
    "    all_max.append(max_id)\n",
    "\n",
    "    if len(ch_en.sents_doc_orig[i]) > 4 and len(ch_fr.sents_doc_tran[max_id]) > 4:\n",
    "        # if len(sent_doc_en[i]) > 4 and len(sent_doc_fr_tran[max_id]) > 4:\n",
    "        all_good_i.append(i)\n",
    "        all_good_max.append(max_id)\n",
    "    else:\n",
    "        # print(f\"skipping {sent_doc_en[i]} or {sent_doc_fr_tran[max_id]}\")\n",
    "        print(f\"skipping {ch_en.sents_doc_orig[i]} or {ch_fr.sents_doc_tran[max_id]}\")\n",
    "\n",
    "    if 0 <= i < 6:\n",
    "        print(max_id)\n",
    "\n",
    "        print(sent_text_en[i][: 120 * 2])\n",
    "        # print(sent_text_en_cheat[i][: 120 * 2])\n",
    "\n",
    "        print(sent_text_fr_tran[max_id][: 120 * 2])\n",
    "\n",
    "        plt.bar(range(win_left, win_right), some_sent_sim)\n",
    "        plt.axvline(ii)\n",
    "        plt.axvline(max_id, c=\"r\")\n",
    "        title = f\"{i} {epub['en'].chapters[ch_id].sent_to_parsent[i]}\"\n",
    "        title += f\" - {ii} {epub['fr'].chapters[ch_id].sent_to_parsent[ii]}\"\n",
    "        title += f\" - {max_id} {epub['fr'].chapters[ch_id].sent_to_parsent[max_id]}\"\n",
    "        plt.title(title)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_max)\n",
    "plt.plot([0, sent_num_en], [0, sent_num_fr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(all_good_i, all_good_max, s=0.1)\n",
    "\n",
    "for par_id, par in enumerate(ch_en.paragraphs):\n",
    "    par_en_start = ch_en.parsent_to_sent[(par_id, 0)]\n",
    "    plt.axvline(par_en_start, linewidth=0.15)\n",
    "\n",
    "\n",
    "for par_id, par in enumerate(ch_fr.paragraphs):\n",
    "    par_fr_start = ch_fr.parsent_to_sent[(par_id, 0)]\n",
    "    plt.axhline(par_fr_start, linewidth=0.15)\n",
    "\n",
    "plt.plot([0, sent_num_en], [0, sent_num_fr], linewidth=0.3)\n",
    "\n",
    "fit_coeff = np.polyfit(all_good_i, all_good_max, 1)\n",
    "fit_func = np.poly1d(fit_coeff)\n",
    "fit_y = fit_func([0, sent_num_en])\n",
    "plt.plot([0, sent_num_en], fit_y)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triang_height = 1\n",
    "triang_filt = triang(win_len * 4 + 1) * triang_height + (1 - triang_height)\n",
    "triang_center = win_len * 2 + 1\n",
    "print(f\"{sent_num_en=} {sent_num_fr=}\")\n",
    "\n",
    "all_max_rescaled = []\n",
    "all_good_i_rescaled = []\n",
    "all_good_max_rescaled = []\n",
    "\n",
    "for i in range(sent_num_en):\n",
    "\n",
    "    # the similarity of this english sent to all the translated ones\n",
    "    this_sent_sim = sim[i]\n",
    "\n",
    "    # find the center rescaled because there are different number of sent in the two chapters\n",
    "    ii = int(i / ratio)\n",
    "\n",
    "    # the chopped similarity array, centered on ii\n",
    "    win_left = max(0, ii - win_len)\n",
    "    win_right = min(sent_num_fr, ii + win_len + 1)\n",
    "    some_sent_sim = this_sent_sim[win_left:win_right]\n",
    "\n",
    "    # the fit along the line\n",
    "    ii_fit = fit_func([i])[0]\n",
    "    ii_fit = int(ii_fit)\n",
    "    if ii_fit < 0:\n",
    "        ii_fit = 0\n",
    "    if ii_fit >= sent_num_fr:\n",
    "        ii_fit = sent_num_fr - 1\n",
    "    # print(f\"{i=} {ii=} {ii_fit=}\")\n",
    "\n",
    "    # chop the filter, centering the apex on the fitted line ii_fit\n",
    "    # the apex is in win_len*2+1\n",
    "    # the similarity is centered on ii\n",
    "    # the shifted filter is still win_len*2+1 long\n",
    "    delta_ii_fit = ii - ii_fit\n",
    "    filt_edge_left = triang_center + delta_ii_fit - win_len - 1\n",
    "    filt_edge_right = triang_center + delta_ii_fit + win_len + 0\n",
    "    triang_filt_shifted = triang_filt[filt_edge_left:filt_edge_right]\n",
    "\n",
    "    # chop the filter as well, if the similarity is near the border\n",
    "    if ii < win_len:\n",
    "        triang_filt_chop = triang_filt_shifted[win_len - ii :]\n",
    "    elif ii > sent_num_fr - (win_len + 1):\n",
    "        left_edge = sent_num_fr - (win_len + 1)\n",
    "        triang_filt_chop = triang_filt_shifted[: -(ii - left_edge)]\n",
    "    else:\n",
    "        triang_filt_chop = triang_filt_shifted\n",
    "\n",
    "    # print( f\"{i=} {ii=} {ii-win_len=} {ii+win_len+1=} {len(some_sent_sim)=} {len(triang_filt_chop)=}\")\n",
    "    assert len(triang_filt_chop) == len(some_sent_sim)\n",
    "\n",
    "    # rescale the similarity\n",
    "    sim_rescaled = some_sent_sim * triang_filt_chop\n",
    "\n",
    "    max_id = all_max[i]\n",
    "\n",
    "    max_id_rescaled = sim_rescaled.argmax() + win_left\n",
    "    all_max_rescaled.append(max_id_rescaled)\n",
    "\n",
    "    # if len(sent_doc_en[i]) > 4 and len(sent_doc_fr_tran[max_id_rescaled]) > 4:\n",
    "    if len(ch_en.sents_doc_orig[i]) > 4 and len(ch_fr.sents_doc_tran[max_id_rescaled]) > 4:\n",
    "        all_good_i_rescaled.append(i)\n",
    "        all_good_max_rescaled.append(max_id_rescaled)\n",
    "\n",
    "    if max_id != max_id_rescaled or False:\n",
    "\n",
    "        diff_tag = \" <><><><>\" if max_id != max_id_rescaled else \"\"\n",
    "        print(f\"{i=} {ii=} {ii_fit=} {max_id=} {max_id_rescaled=}{diff_tag}\")\n",
    "        print(sent_text_en[i][: 120 * 2])\n",
    "        print(sent_text_fr_tran[max_id_rescaled][: 120 * 2])\n",
    "\n",
    "        # if -22 <= i < 260:\n",
    "        x_ii = range(win_left, win_right)\n",
    "        plt.bar(x_ii, some_sent_sim)\n",
    "        plt.bar(x_ii, sim_rescaled)\n",
    "        plt.plot(x_ii, triang_filt_chop)\n",
    "        plt.axvline(ii)\n",
    "        plt.axvline(ii_fit, c=\"r\")\n",
    "        plt.axvline(max_id_rescaled + 0.1, c=\"g\")\n",
    "        title = f\"{i} {epub['en'].chapters[ch_id].sent_to_parsent[i]}\"\n",
    "        title += f\" - {ii} {epub['fr'].chapters[ch_id].sent_to_parsent[ii]}\"\n",
    "        title += f\" - {ii_fit} {epub['fr'].chapters[ch_id].sent_to_parsent[ii_fit]}\"\n",
    "        plt.title(title)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(all_good_i, all_good_max, s=0.9)\n",
    "# plt.scatter(all_good_i_rescaled, all_good_max_rescaled, s=0.9, marker=\"x\")\n",
    "plt.plot(all_good_i_rescaled, all_good_max_rescaled, linewidth=0.9, c=\"C1\")\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InterPOLLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(zip(all_good_i_rescaled, all_good_max_rescaled))\n",
    "\n",
    "is_ooo = []\n",
    "\n",
    "for j, (good_i, good_max_rescaled) in enumerate(\n",
    "    zip(all_good_i_rescaled, all_good_max_rescaled)\n",
    "):\n",
    "\n",
    "    # check for out of order ids\n",
    "    ooo = False\n",
    "\n",
    "    if j == 0:\n",
    "        # only check to the right for the first value\n",
    "        if good_max_rescaled > all_good_max_rescaled[j + 1]:\n",
    "            ooo = True\n",
    "    elif j == len(all_good_max_rescaled) - 1:\n",
    "        # only check to the left for the last value\n",
    "        if good_max_rescaled < all_good_max_rescaled[j - 1]:\n",
    "            ooo = True\n",
    "    else:\n",
    "        if (\n",
    "            good_max_rescaled > all_good_max_rescaled[j + 1]\n",
    "            or good_max_rescaled < all_good_max_rescaled[j - 1]\n",
    "        ):\n",
    "            ooo = True\n",
    "\n",
    "    if ooo:\n",
    "        print(j, good_i, good_max_rescaled)\n",
    "\n",
    "    is_ooo.append(ooo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(all_good_i_rescaled, all_good_max_rescaled, is_ooo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sent_text_en[i][: 120 * 2])\n",
    "# print(sent_text_fr_tran[max_id_rescaled][: 120 * 2])\n",
    "\n",
    "# for s in sent_text_en:\n",
    "#     print(s)\n",
    "#     print()\n",
    "\n",
    "par_id_old = 0\n",
    "for k, sent in enumerate_sent(ch_en, which_sent=\"orig\"):\n",
    "    par_id, sent_id = k\n",
    "    if par_id != par_id_old:\n",
    "        print()\n",
    "        par_id_old = par_id\n",
    "\n",
    "    text_en = sent.text\n",
    "    print(k, text_en)\n",
    "\n",
    "\n",
    "par_id_old = 0\n",
    "for k, sent in enumerate_sent(ch_fr, which_sent=\"orig\"):\n",
    "    par_id, sent_id = k\n",
    "    if par_id != par_id_old:\n",
    "        print()\n",
    "        par_id_old = par_id\n",
    "\n",
    "    text_fr = sent.text\n",
    "    print(k, text_fr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use similarity to pair up sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim[0].shape, sim[:, 0].shape, sim.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('misc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5abe8e27340916d184ac6474958d8876e99dbf694975182a1db18a83bd392dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
